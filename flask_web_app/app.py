# -*- coding: utf-8 -*-
"""predict_diabetes_project_code_6150.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dVt077ySImcRTHXq2ZObISHwDHBuDEX7

# Final project

Prepared By:::

Chetan Kapadia: 801438508

Sanjyot Sathe: 801426514

Mohan Krishna Otikunta: 801418781

Yashodhan Rajesh Jaltare: 801430080

Ravi Kumar 801304869


Dataset Reference:

Kaggle version: https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database

Gist
+2
PMC
+2

UCI (original) distribution: http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data

perso.telecom-paristech.fr
+1

Citation:

Smith, J. W., Everhart, J. E., Dickson, W. C., Knowler, W. C., & Johannes, R. S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. Proceedings of the Symposium on Computer Applications and Medical Care, 261‚Äì265. IEEE Computer Society Press.

Dataset Description:

This dataset contains medical diagnostic measurements for 768 female patients of Pima Indian heritage (age ‚â• 21).
The goal is to predict whether a patient has diabetes (1) or not (0).

| Feature                      | Description                                                                          |
| ---------------------------- | ------------------------------------------------------------------------------------ |
| **Pregnancies**              | Number of times pregnant                                                             |
| **Glucose**                  | Plasma glucose concentration (mg/dL) after 2 hours in an oral glucose tolerance test |
| **BloodPressure**            | Diastolic blood pressure (mm Hg)                                                     |
| **SkinThickness**            | Triceps skin fold thickness (mm)                                                     |
| **Insulin**                  | 2-hour serum insulin (ŒºU/mL)                                                         |
| **BMI**                      | Body mass index (weight in kg / (height in m)¬≤)                                      |
| **DiabetesPedigreeFunction** | Genetic relationship measure (higher = stronger family history)                      |
| **Age**                      | Age in years                                                                         |
| **Outcome**                  | Diabetes diagnosis (1 = yes, 0 = no)                                                 |
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from joblib import dump
from flask import Flask, render_template, request


"""### Utilities

"""

def load_pima_diabetes_data(url: str, column_names: list) -> pd.DataFrame:
    """Loads the Pima Indians Diabetes dataset from a given URL."""
    try:
        df = pd.read_csv(url, names=column_names)
        print("Dataset loaded successfully.")
        return df
    except Exception as e:
        print(f"Failed to load dataset: {e}")
        return pd.DataFrame()  # Return empty DataFrame on failure

def plot_diabetes_outcome_distribution(dataframe):
  # The bar chart shows how many people in our dataset have diabetes versus how many don‚Äôt.
  # It‚Äôs basically a quick way to visualize the balance between the two groups.

    """Plots the distribution of diabetes outcomes using a count plot."""
    plt.figure(figsize=(8, 5))
    sns.countplot(x='Outcome', hue='Outcome', data=dataframe, palette='Set2', legend=False)
    plt.title("Distribution of Diabetes Outcomes", fontsize=14)
    plt.xlabel("Outcome (0 = No Diabetes, 1 = Diabetes)", fontsize=12)
    plt.ylabel("Count", fontsize=12)
    plt.xticks([0, 1], ['No Diabetes', 'Diabetes'])
    plt.tight_layout()
    plt.show()

def plot_feature_correlation_heatmap(dataframe, figsize=(10, 8), cmap='coolwarm'):
    # heatmap that shows how different features (or columns) in a dataset are correlated with each other.
    # A heatmap is a visual representation of data where individual values are represented by colors.
    # In this case, it helps us see the strength of relationships between the columns.

    """Plots a heatmap showing feature correlations in the given DataFrame."""
    correlation_matrix = dataframe.corr()

    plt.figure(figsize=figsize)
    sns.heatmap(correlation_matrix, annot=True, cmap=cmap, fmt=".2f", linewidths=0.5, square=True)
    plt.title("Feature Correlation Heatmap", fontsize=14)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.show()

def plot_pairwise_feature_relationships(dataframe, features, hue='Outcome', diag_kind='kde', title="Pairwise Feature Relationships"):
    # a pairwise plot that visualizes how multiple features in your dataset relate to each other.
    # It uses Seaborn's pairplot, which is a great way to explore relationships between pairs of features in a dataset.

    """Plots pairwise relationships among selected features using Seaborn's pairplot."""
    sns.set(style="whitegrid")
    pairplot = sns.pairplot(dataframe[features], hue=hue, diag_kind=diag_kind, palette='Set2')
    pairplot.fig.suptitle(title, y=1.02, fontsize=14)
    plt.show()

def plot_age_vs_glucose(dataframe, x='Age', y='Glucose', hue='Outcome', palette='coolwarm', figsize=(10, 6)):
    """Plots a scatter plot of Age vs Glucose, colored by diabetes outcome."""
    plt.figure(figsize=figsize)
    sns.scatterplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)
    plt.title("Age vs Glucose (Colored by Diabetes Outcome)", fontsize=14)
    plt.xlabel(x, fontsize=12)
    plt.ylabel(y, fontsize=12)
    plt.legend(title=hue)
    plt.tight_layout()
    plt.show()

# prepare_data_for_modeling function as our data prep assistant before building a machine learning model.

#It takes our raw dataset and gets it ready for training by:

# Splitting it into training and testing sets.

# Scaling the features so they‚Äôre all on the same playing field.

def prepare_data_for_modeling(dataframe, target_column='Outcome', test_size=0.2, random_state=42):
    """
    Splits the dataset into training and testing sets, and standardizes the features.

    Parameters:
        dataframe (pd.DataFrame): The input dataset.
        target_column (str): The name of the target column.
        test_size (float): Proportion of the dataset to include in the test split.
        random_state (int): Seed for reproducibility.

    Returns:
        X_train_scaled, X_test_scaled, y_train, y_test
    """
    # Separate features and target
    #X: the input features (like age, glucose, BMI‚Ä¶)
    #y: the target we want to predict (in this case, whether someone has diabetes)
    X = dataframe.drop(columns=[target_column])
    y = dataframe[target_column]

    # Split into training and testing sets
    # Training set (80%): used to teach the model
    # Testing set (20%): used to see how well the model performs
    # The stratify=y part makes sure both sets have a balanced mix of diabetic and non-diabetic cases.
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )

    # Standardize features
    # It standardizes the features so they all have the same scale ‚Äî
    # this is super important for models like KNN that rely on distances between points.
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    return scaler, X_train_scaled, X_test_scaled, y_train, y_test

def evaluate_knn(X_train, y_train, X_test, y_test, max_k=20):

    """Evaluate KNN accuracy for k=1..max_k and return scores + best k."""
    k_values = range(1, max_k + 1)
    scores = [
        accuracy_score(y_test, KNeighborsClassifier(n_neighbors=k)
                       .fit(X_train, y_train)
                       .predict(X_test))
        for k in k_values
    ]

    # Plot results
    plt.figure(figsize=(8, 5))
    plt.plot(k_values, scores, marker='o')
    plt.xlabel('Number of Neighbors (k)')
    plt.ylabel('Accuracy')
    plt.title('KNN Accuracy for Different k Values')
    plt.grid(True)
    # plt.show()

    # Best k
    best_k = k_values[scores.index(max(scores))]
    print(f"Best k value: {best_k} (Accuracy: {max(scores):.4f})")
    return scores, best_k

#train_knn_classifier function is like our personal coach for building a K-Nearest Neighbors (KNN) model.
#It takes our training data and teaches the model how to recognize patterns
#‚Äî so later, it can predict whether someone has diabetes based on their health features.

def train_knn_classifier(X_train, y_train, n_neighbors=5):
    """
    Trains a K-Nearest Neighbors classifier on the provided training data.

    Parameters:
        X_train (array-like): Scaled training features.
        y_train (array-like): Training labels.
        n_neighbors (int): Number of neighbors to use (default is 5).

    Returns:
        knn (KNeighborsClassifier): Trained KNN model.
    """

    #X_train: This is our input data ‚Äî things like glucose level, age, BMI, etc.
    #y_train: These are the correct answers ‚Äî whether each person has diabetes (1) or not (0).
    #n_neighbors=5: This tells the model to look at the 5 closest neighbors when making a prediction.
    knn = KNeighborsClassifier(n_neighbors=n_neighbors)
    knn.fit(X_train, y_train)
    return knn

# make_knn_predictions function is like your KNN model‚Äôs crystal ball ‚Äî
# it uses the trained model to make predictions on new data. It tells us:

# Who it thinks has diabetes (based on the test data)

# How confident it is in those predictions

def make_predictions(model, X_test):
    """
    Generates predictions and probability scores using a trained KNN model.

    Parameters:
        model (KNeighborsClassifier) / Logistic Regression / Random Forest: Trained model.
        X_test (array-like): Scaled test features.

    Returns:
        y_pred (array): Predicted class labels.
        y_prob (array): Predicted probabilities for the positive class (used for ROC-AUC).
    """

    #This predict function asks the model to predict the outcome (diabetes or not)
    #for each person in the test set.
    #We get a list of 0s and 1s ‚Äî 0 means no diabetes, 1 means diabetes.
    y_pred = model.predict(X_test)

    #predict_proba function gives us the probability that each person has diabetes.
    #It‚Äôs useful when we want to measure how confident the model is ‚Äî especially for metrics like ROC-AUC.
    y_prob = model.predict_proba(X_test)[:, 1]  # Probability of class 1 (diabetes)

    return y_pred, y_prob

#train_logistic_regression Takes in training data (X_train, y_train).

# Builds a Logistic Regression model with safe defaults (like more iterations so it doesn‚Äôt quit too early).

# Fits the model so it learns patterns from our data.

# Returns the trained model so we can use it later to predict outcomes or check accuracy.

def train_logistic_regression(X_train, y_train, max_iter=1000, random_state=42):
    """Train a Logistic Regression model with scaling already applied."""
    model = LogisticRegression(max_iter=max_iter, random_state=random_state)
    model.fit(X_train, y_train)
    return model

# train_random_forest is our shortcut to grow a forest of decision trees,
# train them on our data, and get a strong, reliable model back.

def train_random_forest(X_train, y_train, n_estimators=100, random_state=42, max_depth=None):
    """
    Train a Random Forest classifier.

    Parameters:
        X_train (array-like): Training features (already scaled if needed)
        y_train (array-like): Training labels
        n_estimators (int): Number of trees in the forest
        random_state (int): Seed for reproducibility
        max_depth (int or None): Maximum depth of the trees

    Returns:
        model: Trained RandomForestClassifier
    """
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        random_state=random_state,
        max_depth=max_depth
    )
    model.fit(X_train, y_train)
    return model

#evaluate_model_performance function is like our model‚Äôs report card.
#After our model makes predictions,
#this function checks how well it did ‚Äî
#using five key metrics that tell us if our model is smart, fair, and reliable.

#It gives us a quick snapshot of your model‚Äôs performance.

#We can compare models easily using the same metrics.

#It helps us spot weaknesses ‚Äî like if your model misses too many real cases (low recall).

def evaluate_model_performance(y_true, y_pred, y_prob):
    """
    Evaluates and prints key classification metrics for a model.

    Parameters:
        y_true (array-like): True labels.
        y_pred (array-like): Predicted labels.
        y_prob (array-like): Predicted probabilities for the positive class.
    """

  #Accuracy: How often the model got it right overall.

  #Precision: When it said ‚Äúdiabetes,‚Äù how often was it correct?

  #Recall: Out of all actual diabetes cases, how many did it catch?

  #F1-score: A balance between precision and recall ‚Äî great for uneven datasets.

  #ROC-AUC: Measures how well the model separates diabetic vs non-diabetic cases using probability scores.

    metrics = {
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred),
        "Recall": recall_score(y_true, y_pred),
        "F1-score": f1_score(y_true, y_pred),
        "ROC-AUC": roc_auc_score(y_true, y_prob)
    }

    for name, value in metrics.items():
        print(f"{name}: {value:.3f}")

# display_classification_report function is like our model‚Äôs performance summary sheet.
# After our model makes predictions, it prints out a detailed report showing how well it did ‚Äî
# not just overall, but for each class (like diabetic vs non-diabetic).
def display_classification_report(y_true, y_pred, target_names=None):
    """
    Prints a formatted classification report showing precision, recall, f1-score, and support.

    Parameters:
        y_true (array-like): True labels.
        y_pred (array-like): Predicted labels.
        target_names (list, optional): Names for target classes (e.g., ['No Diabetes', 'Diabetes']).
    """

    # This generates and prints a table that shows:

    # Precision: How often the model was right when it predicted a class.

    # Recall: How many actual cases the model correctly identified.

    # F1-score: A balance between precision and recall.

    # Support: How many examples were in each class.

    # If we provide target_names, it replaces the default 0/1 labels
    # with something more readable like ‚ÄúNo Diabetes‚Äù and ‚ÄúDiabetes.‚Äù
    print("\nüìã Classification Report:\n")
    report = classification_report(y_true, y_pred, target_names=target_names)
    print(report)

def plot_confusion_matrix(y_true, y_pred, title="Confusion Matrix - KNN Classifier", cmap='Blues'):

    # confusion matrix heatmap, which is a handy way to see how well a classification model (like a KNN classifier) is performing.
    # A confusion matrix basically compares what your model predicted versus what the actual answers were ‚Äî and shows the results in a simple grid.

    """
    Plots a heatmap of the confusion matrix for classification results.

    Parameters:
        y_true (array-like): True labels.
        y_pred (array-like): Predicted labels.
        title (str): Title of the plot.
        cmap (str): Color map for the heatmap.
    """
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False)
    plt.title(title, fontsize=14)
    plt.xlabel("Predicted", fontsize=12)
    plt.ylabel("Actual", fontsize=12)
    plt.tight_layout()
    plt.show()

"""## Load data and preprocessing"""

# Define dataset URL and column names
DATA_URL = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
COLUMN_NAMES = [
    'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',
    'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'
]

# Load the dataset
df = load_pima_diabetes_data(DATA_URL, COLUMN_NAMES)

print(df.head())

print(df.describe())
print(df['Outcome'].value_counts())

# plot_diabetes_outcome_distribution(df)

"""# Feature Correlation Heatmap
graph helps us identify features that might be unnecessary or closely connected.
It also assists with feature engineering and model tuning.
Plus, it makes your data exploration more intuitive and visually appealing.

Dark red = strong positive correlation (close to +1)

Dark blue = strong negative correlation (close to ‚Äì1)

White/light colors = weak or no correlation (close to 0)
"""

# plot_feature_correlation_heatmap(df)

"""#Pairwise Feature Relationships
graph is a grid of scatter plots and histograms ‚Äî to help us explore how different features in our dataset relate to each other. It‚Äôs like speed dating for our data: every feature gets a chance to meet every other feature and show how they interact!

We can spot patterns ‚Äî like whether higher glucose levels are linked to diabetes.

We can see clusters or overlaps between groups.

It‚Äôs a great way to visually explore our data before building models.
"""

# Define features to include
selected_features = ['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'Outcome']

# Call the function
# plot_pairwise_feature_relationships(df, selected_features)

"""#Scatter Plot: Age vs Glucose
graph Tshows how a person‚Äôs age relates to their glucose level ‚Äî and it adds a splash of color to show whether or not they have diabetes. It‚Äôs like putting our data on a map and using color to highlight patterns.

It‚Äôs a quick way to visualize relationships in our data.

We can spot clusters or outliers.

It helps us to understand patterns before building a predictive model.
"""

# plot_age_vs_glucose(df)

"""## KNN - build model, train&test, performance"""

scaler, X_train_scaled, X_test_scaled, y_train, y_test = prepare_data_for_modeling(df)

scores, best_k = evaluate_knn(X_train_scaled, y_train, X_test_scaled, y_test)

knn_model = train_knn_classifier(X_train_scaled, y_train,best_k)

dump(knn_model, 'diabetes_knn_model.joblib')

y_pred, y_prob = make_predictions(knn_model, X_test_scaled)

evaluate_model_performance(y_test, y_pred, y_prob)

display_classification_report(y_test, y_pred, target_names=['No Diabetes', 'Diabetes'])

# plot_confusion_matrix(y_test, y_pred)

"""## Regression Logistic Classifier - build model, train&test, performance"""

logistic_regression_model = train_logistic_regression(X_train_scaled, y_train)

dump(logistic_regression_model, 'diabetes_logistic_regression_model.joblib')

y_pred, y_prob = make_predictions(logistic_regression_model, X_test_scaled)

evaluate_model_performance(y_test, y_pred, y_prob)

display_classification_report(y_test, y_pred, target_names=['No Diabetes', 'Diabetes'])

# plot_confusion_matrix(y_test, y_pred)

"""## Random Forest - build model, train&test, performance

"""

rf_model = train_random_forest(X_train_scaled, y_train)

dump(rf_model, 'diabetes_rf_model.joblib')

y_pred, y_prob = make_predictions(rf_model, X_test_scaled)

evaluate_model_performance(y_test, y_pred, y_prob)

display_classification_report(y_test, y_pred, target_names=['No Diabetes', 'Diabetes'])

# plot_confusion_matrix(y_test, y_pred)

# Input feature names (must match the training DataFrame!)
feature_names = [
    "Pregnancies", "Glucose", "BloodPressure", "SkinThickness",
    "Insulin", "BMI", "DiabetesPedigreeFunction", "Age"
]

# User input
# user_data = np.array([[2, 120, 70, 25, 100, 28.0, 0.35, 29]])
# user_data = np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50]])
# Convert to DataFrame with correct column names
user_df = pd.DataFrame(user_data, columns=feature_names)

# Scale using the SAME trained scaler
user_data_scaled = scaler.transform(user_df)

# Predict
prediction = knn_model.predict(user_data_scaled)

# Interpret result
if prediction[0] == 1:
    print("üî¥ The KNN model predicts: Diabetic")
else:
    print("üü¢ The KNN model predicts: Not Diabetic")

# Predict
prediction = logistic_regression_model.predict(user_data_scaled)

# Interpret result
if prediction[0] == 1:
    print("üî¥ The Logistic Regression model predicts: Diabetic")
else:
    print("üü¢ The Logistic Regression model predicts: Not Diabetic")

# Predict
prediction = rf_model.predict(user_data_scaled)

# Interpret result
if prediction[0] == 1:
    print("üî¥ The Random Forest model predicts: Diabetic")
else:
    print("üü¢ The Random Forest model predicts: Not Diabetic")


# Initialize Flask app
app = Flask(__name__)

# Route for the homepage
@app.route('/')
def home():
    return render_template('index.html')

# Route to handle form submission and prediction
@app.route('/predict', methods=['POST'])
def predict():
    # Get the form data from the user
    pregnancies = float(request.form['pregnancies'])
    glucose = float(request.form['glucose'])
    blood_pressure = float(request.form['blood_pressure'])
    skin_thickness = float(request.form['skin_thickness'])
    insulin = float(request.form['insulin'])
    bmi = float(request.form['bmi'])
    pedigree = float(request.form['pedigree'])
    age = float(request.form['age'])

    # # Create a numpy array of the input features
    input_features = np.array([[pregnancies, glucose, blood_pressure, skin_thickness, insulin, bmi, pedigree, age]])

    # # Standardize the features (same as when training the model)
    scaler = StandardScaler()
    input_scaled = scaler.fit_transform(input_features)

    feature_names = [
        "Pregnancies", "Glucose", "BloodPressure", "SkinThickness",
        "Insulin", "BMI", "DiabetesPedigreeFunction", "Age"
    ]

    # User input
    user_data =  np.array([[pregnancies, glucose, blood_pressure, skin_thickness, insulin, bmi, pedigree, age]])
    
    # user_data = np.array([[2, 120, 70, 25, 100, 28.0, 0.35, 29]])
    # user_data = np.array([[6, 148, 72, 35, 0, 33.6, 0.627, 50]])
    # Convert to DataFrame with correct column names
    user_df = pd.DataFrame(user_data, columns=feature_names)

    # Scale using the SAME trained scaler
    user_data_scaled = scaler.transform(user_df)

    # Predict
    prediction = knn_model.predict(user_data_scaled)

    # Make the prediction using the trained model
    knn_prediction = knn_model.predict(input_scaled)
    logistic_regression_prediction = logistic_regression_model.predict(input_scaled)
    rf_prediction = rf_model.predict(input_scaled)


    # Interpret the result
    if knn_prediction[0] == 1:
        result = "You are at risk of diabetes."
    else:
        result = "You are not at risk of diabetes."

    return render_template('index.html'
                           , prediction_text=result
                           , pregnancies=pregnancies
                           , glucose = glucose
                           , blood_pressure = blood_pressure
                           , skin_thickness = skin_thickness
                           , insulin = insulin
                           , bmi = bmi
                           , pedigree = pedigree
                           , age = age
                           , knn_prediction = knn_prediction
                           , logistic_regression_prediction = logistic_regression_prediction
                           , rf_prediction = rf_prediction)

if __name__ == "__main__":
    app.run(debug=True)